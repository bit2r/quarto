---
title: "한글 인코딩"
author:
  - name: 이광춘
    url: https://www.linkedin.com/in/kwangchunlee/
    affiliation: 한국 R 사용자회
    affiliation-url: https://github.com/bit2r
title-block-banner: true
#title-block-banner: "#562457"
format:
  html:
    theme: flatly
    code-fold: true
    toc: true
    toc-depth: 3
    toc-title: 목차
    number-sections: true
    highlight-style: github    
    self-contained: false
bibliography: bibliography.bib
link-citations: yes
csl: apa-single-spaced.csl
filters:
   - lightbox
lightbox: auto
editor_options: 
  chunk_output_type: console
execute:
  warning: false
  message: false  
---
 
# 웹 표준 인코딩

스마트폰의 대중화에 따라 더이상 윈도우 운영체제에서 사용되는 문자체계가 더이상 표준이 되지 못하고 여러 문제점을 야기함에 따라 **유니코드 + UTF-8** 체제가 대세로 자리잡고 있는 것이 확연히 나타나고 있다.

2010년 구글에서 [발표](https://googleblog.blogspot.com/2010/01/unicode-nearing-50-of-web.html)한 자료에 의하면 2010년 UTF-8 인코딩이 웹에서 주류로 부상하기 시작한 것이 확인되었다. [@unicode2010] 웹기반 플롯 디지털 도구를 활용하여 그래프([WebPlotDigitizer](https://apps.automeris.io/wpd/))에서 데이터를 추출하여 시각화면 유사한 결과를 시각적으로 표현할 수 있다. 

[데이터 출처: [Historical yearly trends in the usage statistics of character encodings for websites](https://w3techs.com/technologies/history_overview/character_encoding/ms/y)]{.aside}

2010년 이후 웹에서 가장 점유율이 높은 인코딩 방식은 UTF-8으로 W3Tech Web Technology Surveys 를 통해 확인을 할 수 있다.


```{r}
library(tidyverse)
extrafont::loadfonts()

## 1. 2000년부터 웹 인코딩 추세 ------------------
ascii <- read_csv("data/ascii_red.csv", col_names = FALSE) %>% 
  set_names(c("연도", "ascii"))

ascii_tbl <- ascii %>% 
  mutate(연도 = floor(연도)) %>% 
  group_by(연도) %>% 
  summarize(ascii = mean(ascii))

iso_8859 <- read_csv("data/iso_orange.csv", col_names = FALSE) %>% 
  set_names(c("연도", "iso_8859"))

iso_8859_tbl <- iso_8859 %>% 
  mutate(연도 = floor(연도)) %>% 
  group_by(연도) %>% 
  summarize(iso_8859 = mean(iso_8859))

utf_8 <- read_csv("data/utf-8_blue.csv", col_names = FALSE) %>% 
  set_names(c("연도", "utf_8"))

utf_8_tbl <- utf_8 %>% 
  mutate(연도 = floor(연도)) %>% 
  group_by(연도) %>% 
  summarize(utf_8 = mean(utf_8, na.rm = FALSE)) 

encoding_tbl <- left_join(ascii_tbl, iso_8859_tbl) %>% 
  left_join(utf_8_tbl) %>% 
  pivot_longer(cols = -연도) %>% 
  mutate(value = ifelse(is.na(value), 0, value)) %>% 
  filter(연도 <= 2009)

encoding_2010_g <- encoding_tbl %>% 
  mutate(연도 = lubridate::make_date(year = 연도)) %>% 
  ggplot(aes(x = 연도, y = value, color = name)) +
    geom_line() +
    geom_point() +
    theme_bw(base_family = "MaruBuri") +
    labs(x = "",
         y = "점유율(%)",
         title = "웹에서 UTF-8 성장세",
         subtitle = "2010 ~ 2010년",
         color = "인코딩") +
    scale_y_continuous(labels = scales::percent) +
    scale_color_manual(values = c("ascii" = "gray50", "iso_8859" = "red", "utf_8" = "blue")) +
  theme(legend.position = "top") +
  expand_limits(y = c(0, 1))

## 2. 2010년부터 웹 인코딩 추세 ------------------
library(readxl)

encoding_raw <- read_excel("data/web_encoding.xlsx", 
                           col_types = "text")

encoding_web <- encoding_raw %>% 
  janitor::clean_names(ascii = FALSE) %>% 
  pivot_longer(cols = -구분, names_to = "연도", values_to = "점유율") %>% 
  mutate(연도 = str_extract(연도, "\\d{4}") %>% as.integer(.)) %>% 
  mutate(점유율 = parse_number(점유율)) %>% 
  group_by(구분, 연도) %>% 
  summarise(점유율 = min(점유율, na.rm = TRUE)) %>% 
  ungroup()

encoding_2022_g <- encoding_web %>% 
  filter(구분 %in% c("EUC-KR", "GB2312", "Shift JIS", "UTF-8", "ISO-8859-1")) %>% 
  mutate(연도 = lubridate::make_date(year = 연도)) %>%   
  ggplot(aes(x = 연도, y = 점유율, color = 구분)) +
    geom_line() +
    geom_point() +
    theme_bw(base_family = "MaruBuri") +
    labs(x = "",
         y = "점유율(%)",
         title = "웹에서 UTF-8 성장세",
         subtitle = "2011 ~ 2012년 3분기",
         color = "인코딩") +
    scale_y_continuous(labels = scales::percent) +
    scale_color_manual(values = c("EUC-KR" = "gray50", "GB2312" = "gray80", "Shift JIS" = "gray70", "ISO-8859-1" = "red", "UTF-8" = "blue")) +
  theme(legend.position = "top")   +
  expand_limits(y = c(0, 1))

## 3. 시각화 요약 ------------------
library(patchwork)

encoding_2010_g + encoding_2022_g
```


# 유니코드와 UTF-8

사람과 사람 사이 의사소통하는데 필요한 것 중 하나가 **문자**다. 문자는 영어 알파벳, 한국어 한글, 중국 한자 등 무수히 많은 언어가 문자를 사용해서 의사를 소통한다. 이를 컴퓨터를 활용해서 의사소통하는 경우 컴퓨터가 인식할 수 있도록 바이트를 사용한다.

컴퓨터는 바이트로 만들어 졌고, 바이트를 묶어 파일이 되고, 파일을 네트워크에 전송해서 다른 컴퓨터에 넘겨야 되고, 이를 사람이 읽을 수 있는 형태로 변환하면 의사소통이 완성된다. 이 과정에서 인코딩(Encoding, 부호화)하는 과정과 디코딩(Decoding, 복화화)하는 과정을 거치게 된다.


- 컴퓨터에서 입출력되는 모든 정보는 바이트(Byte), 즉 예를 들어 알파벳 `A`는 `0100 0001`으로 부호화되어 전달된다.
- `0100 0001` 바이트를 사람이 인식할 수 있는 `A`로 인식한 최초의 약속이 **아스키(ASCII)** 다. 
- 영문자를 기반으로한 ASCII 256개 기호보다 많은 기호를 전달할 필요가 있다. 특히, CJK 동아시아 한자문화권에서는 더욱 그렇다. 그렇게 컴퓨터를 이용하여 문자를 전달하고 전달받고자하는 영문자를 제외한 다른 문화권에서 이를 확장한 노력이 동시 다발적으로 일어났고 한글은 EUC-KR, 일본문자는 Shift_JIS, 중국 한자는 GB2312, 러시아 등에서 사용되는 키릴 문자는 Windows-1251, 같은 라틴문화권이지만 각자 고유 문자를 갖고 있는 스페인, 프랑스, 독일 등은 Windows-1252 규약을 사용했다. 
- 표의문자(이모지 등)가 대거 등장하고 아랍어를 비롯한 전세계 문자를 컴퓨터에 넣기 위해서 더 많은 문자를 담을 수 있는 표준이 필요했고 **유니코드(Unicode)** 가 탄생했고 이제 웹 표준으로 정착됨은 물론 모든 문서의 문자를 표현하는 표준으로 자리잡아가고 있다.

![유니코드와 UTF-8](images/encoding_unicode.png)

**유니코드(Unicode)**는 글자와 코드가 1:1 매핑되어 있는 단순한 **코드표**에 불과하고 산업표준으로 일종의 국가 당사자간 약속이다. **한글**이 표현된 유니코드 영역은 [위키백과 
유니코드 영역](https://ko.wikipedia.org/wiki/%EC%9C%A0%EB%8B%88%EC%BD%94%EB%93%9C_%EC%98%81%EC%97%AD)에서 찾을 수 있다.

**UTF-8(Universal Coded Character Set + Transformation Format – 8-bit의 약자)**은 앞서 정의한 유니코드를 위한 가변 길이 문자 인코딩 방식 중 하나로, 켄 톰프슨과 롭 파이크가 제작했다. 

예를 들어, 영어 `A` 대문자는 1 바이트, 한글 `가`는 3 바이트다.

```{r}
A2Bit <- pryr::bits("A", split = TRUE)
A2Bit
length(A2Bit)

Ga2Bit <- pryr::bits("가", split = TRUE)
Ga2Bit
length(Ga2Bit)
```


# R 4.2

R 4.2 버전은 그 이전 R과 달리 크게 변화된 문자체계를 갖고 있다. R 언어로 통계 및 데이터 과학 작업을 할 경우 크게 3개 문자표(Character Map)가 중요하다. 사실 다른 언어권의 문자도 유사하게 생각하여 확장할 수도 있다.

[Alex Farach, "Let's start at the beginning - bits to character encoding in R", rstudio::conf(2022)]{.aside}

- ASCII: 7 비트 = $2^7 = 128$ 문자
- Latin1 (ISO-8859-1): 8 비트 = $2^8 = 256$ 문자
- UTF-8 (Unicode Transformation 8-bit): 1:4 바이트 = 1,112,064 문자(혹은, Code Points)

:::{.callout-note}
**MBCS(Multi-Byte Character Set)**는 바이트 2개이상을 사용해서 문자를 표현하는 인코딩 방식으로 2 바이트를 사용하는 **DBCS(double-byte character set)**으로 한글, 일본문자, 한자(CJK)를 표현하는데 큰 무리가 없었기 때문에 MBCS = DBCS 로 볼 수 있었다. MBCS는 **UNICODE**라는 산업표준이 정립되기 이전에 CJK 문자를 표현하기 위한 하나의 과도기적 문자표로 이해할 수 있다.
:::

```{r}
coffee_v <- c("커피", "coffee", "café", "caf\u00E9", "caf\xe9")
print(coffee_v)
Encoding(coffee_v)
```

R 4.2 이후 버전에서 `Encoding()` 함수를 사용해서 인코딩을 확인할 수 있다. 크게 보면 기존  

```{r}
l10n_info()
```

MBCS와 UTF-8 이 활성화되어 있지만, Latin-1 즉 확장 아스키 코드는 비활성화되어 있고 코드페이지는 65001을 갖고 있다.

```{r}
Sys.getlocale()
```

**로컬라이제이션(localization)**은 사용자의 문화와 언어에 맞추는 일체의 과정을 의미하고 당연히 문자도 여기에 포함된다. 날짜, 시간, 숫자 표기법 등이 여기에 포함된다. 로컬라이제이션이 영문자로도 길기 때문에 줄여서 `l10n`으로 표현한다.

- LC_ALL: 모든 카테고리에 대한 로케일 설정을 위한 환경변수
- LC_CTYPE: 문자 분류, 글자수, 대소문자 구분
- LC_NUMERIC: 숫자와 관련된 기준
- LC_COLLATE: 문자열의 정렬 순서를 결정
- LC_TIME: 날짜 시간 표시방법
- LC_MONETARY: 통화나 금액과 관련된 숫자의 기준
- LC_MESSAGES: 메시지 표시
- LC_PAPER: 종이
- LC_ADDRESS: 주소
- LC_TELEPHONE: 전화번호
- LC_MEASUREMENT: 측정단위 (무게, 온도 등)
- LC_NAME: ???
- LC_IDENTIFICATION: ???



# 문자표

**ASCII**는 7비트로 총 $2^7=128$ 문자 주로 영미권 문자를 문자표(Character Map)에 매핑하는데 사용되는 인코딩 방식이다. 영어 파운드화 등 기호를 확장하여 인접한 라틴문화권 문자도 컴퓨터로 표시할 수 있도록 확장한 표준이 **ISO-8859-1**로 8비트를 사용하여  총 $2^8=256$개 문자 표현이 가능하다.

요약하면, 영문자(Alphabet)를 표현하는데 ASCII 7비트 총 128개 문자로 충분했으나 추가 문자가 필요한 언어의 경우 이를 확장하여 256개 문자를 각자 나머지 128개 문자를 각자 문자에 넣어 표준을 제정하여 사용했었다.

- EUC-KR: 한글
- Shift_JIS : 일본문자
- GB2312: 한자
- Windows-1251: 키릴 문자 (러시아 등)
- Windows-1252: 스페인, 프랑스, 독일 등
- ...

과거 문자를 단순히 영미권과 주요 선진국에서 사용하던 것을 넘어 전세계 문자를 컴퓨터로 표현하는데 유니코드가 제정되었고 웹에서 빠르게 국제표준으로 자리잡았다.

## ASCII 문자

**ASCII** 총 $2^7=128$ 문자 중 제어문자를 제외한 출력가능한 문자를 뽑아보자.

```{r}
coderange <- c(33:126)

ascii_table <- tibble(
 십진수   = coderange,
 십육진수 = as.raw(coderange),
 문자     = rawToChar(as.raw(coderange), multiple=TRUE),
 이진수   = pryr::bits(문자, split = TRUE)
) %>% 
  relocate(문자, .before = 십진수)

ascii_table %>% 
  slice_sample(n=30)
```

출력가능한 ASCII 문자표를 33번부터 126번까지 **문자 십진수 이진수** 순으로 출력하면 다음과 같다.

```{r}
library(mmtable2) # devtools::install_github("ianmoran11/mmtable2")

ascii_table %>% 
  mutate(index = row_number()) %>% 
  mutate(x_grp = index %/% 10 ) %>%
  mutate(y_grp = index %% 10 ) %>% 
  group_by(x_grp, y_grp) %>%
  summarise(문자 = glue::glue("{문자} {십진수} {이진수}") ) %>%
  ungroup() %>%
  mmtable(cells = 문자) +
  header_left(x_grp) +
  header_top(y_grp)
```

## 확장 ASCII 문자

확장 **ASCII** 총 $2^8=256$ 문자 중 앞선 기본 ASCII를 제외한 나머지 확장문자표를 출력해보자.

```{r}
coderange <- c(161:255)
# iconv(extendedascii, from="Windows-1252", to="UTF-8") 

extended_ascii_table <- tibble(
 십진수   = coderange,
 십육진수 = as.raw(coderange),
 문자     = rawToChar(as.raw(coderange), multiple=TRUE),
 이진수   = pryr::bits(문자, split = TRUE)
) %>% 
  relocate(문자, .before = 십진수) %>% 
  mutate(문자 = str_conv(문자, encoding = "latin-1")) 

extended_ascii_table  %>% 
  slice_sample(n=30)
```

```{r}
library(mmtable2) # devtools::install_github("ianmoran11/mmtable2")

extended_ascii_table %>% 
  mutate(index = row_number()) %>% 
  mutate(x_grp = index %/% 10 ) %>%
  mutate(y_grp = index %% 10 ) %>% 
  group_by(x_grp, y_grp) %>%
  summarise(문자 = glue::glue("{문자}\n{십진수}\n {이진수}") ) %>%
  ungroup() %>%
  ggplot(aes(x=x_grp, y=y_grp)) +
    geom_text(aes(label = 문자), size = 3) +
    theme_void(base_family = "MaruBuri")
```

[참조: [Ascii code table in R](https://www.r-bloggers.com/2011/03/ascii-code-table-in-r/)],
[ASCII Table with All 256 Character codes in decimal, hexadecimal, octal and binary](https://www.sciencebuddies.org/science-fair-projects/references/ascii-table){.aside}

## 유니코드

[`Unicode`](https://cran.r-project.org/web/packages/Unicode/index.html) 패키지를 통해서 유니코드에 등록된 문자를 확인할 수 있다.

```{r}
library(Unicode)
library(rlang)

all_scripts <- u_scripts()
names(all_scripts)
```

### 영문자

영문자를 유니코드에서 찾아 살펴보자. 영문자는 라틴 계열이라 라틴계열에 포함된 문자를 뽑아서 유니코드 범위를 정해서 유니코드 범위에 포함된 문자를 사람이 읽을 수 있는 문자로 화면에 일부만 출력한다.

[코드 참조: [How to generate all possible unicode characters?](https://stackoverflow.com/questions/71587307/how-to-generate-all-possible-unicode-characters)]{.aside}


```{r}
latin_char <-  u_scripts('Latin')

latin_char_length <- map(latin_char, n_of_u_chars)

show_characters <- function(i) {
  expand_ranges <- as.u_char_seq(latin_char$Latin[i])
  unicode_chars <- unlist(expand_ranges)
  intToUtf8(unicode_chars)
}

map_chr(1:10, show_characters)
```

### 한글

동일하게 유니코드에 등록된 한글도 확인해보자.
한글완성형으로 한글 유니코드 7번째 블록에 11,172 개 글자가 등록되어 있어 이를 제외한 한글을 뽑아보자.

```{r}
hangul_scripts <- u_scripts('Hangul')

hangul_char_length <- map(hangul_scripts, n_of_u_chars)

map(hangul_scripts, n_of_u_chars)

expand_ranges <- as.u_char_seq(hangul_scripts$Hangul[7])

show_hangul <- function(i) {
  expand_ranges <- as.u_char_seq(hangul_scripts$Hangul[i])
  unicode_chars <- unlist(expand_ranges)
  intToUtf8(unicode_chars)
}

map_chr(c(1:6,8:14), show_hangul)
```

:::{.callout-note}

`rlang` 패키지 `chr_unserialise_unicode()` 함수는 유니코드를 문자로 변환하여 확인할 때 유용하다.

```{r}
library(rlang)
chr_unserialise_unicode('<U+AC00>')
chr_unserialise_unicode('<U+FFDC>')
```
:::